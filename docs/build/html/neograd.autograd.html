
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>neograd.autograd package &#8212; neograd  documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css" />
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/sphinx_highlight.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="neograd.autograd.ops package" href="neograd.autograd.ops.html" />
    <link rel="prev" title="neograd package" href="neograd.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="neograd-autograd-package">
<h1>neograd.autograd package<a class="headerlink" href="#neograd-autograd-package" title="Permalink to this heading">¶</a></h1>
<section id="subpackages">
<h2>Subpackages<a class="headerlink" href="#subpackages" title="Permalink to this heading">¶</a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="neograd.autograd.ops.html">neograd.autograd.ops package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="neograd.autograd.ops.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="neograd.autograd.ops.html#module-neograd.autograd.ops.activations">neograd.autograd.ops.activations module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.activations.ReLU"><code class="docutils literal notranslate"><span class="pre">ReLU</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.activations.ReLU.backward"><code class="docutils literal notranslate"><span class="pre">ReLU.backward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.activations.ReLU.forward"><code class="docutils literal notranslate"><span class="pre">ReLU.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.activations.Sigmoid"><code class="docutils literal notranslate"><span class="pre">Sigmoid</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.activations.Sigmoid.backward"><code class="docutils literal notranslate"><span class="pre">Sigmoid.backward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.activations.Sigmoid.forward"><code class="docutils literal notranslate"><span class="pre">Sigmoid.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.activations.Softmax"><code class="docutils literal notranslate"><span class="pre">Softmax</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.activations.Softmax.backward"><code class="docutils literal notranslate"><span class="pre">Softmax.backward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.activations.Softmax.calc_softmax"><code class="docutils literal notranslate"><span class="pre">Softmax.calc_softmax()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.activations.Softmax.forward"><code class="docutils literal notranslate"><span class="pre">Softmax.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.activations.Tanh"><code class="docutils literal notranslate"><span class="pre">Tanh</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.activations.Tanh.backward"><code class="docutils literal notranslate"><span class="pre">Tanh.backward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.activations.Tanh.forward"><code class="docutils literal notranslate"><span class="pre">Tanh.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.activations.relu"><code class="docutils literal notranslate"><span class="pre">relu()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.activations.sigmoid"><code class="docutils literal notranslate"><span class="pre">sigmoid()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.activations.softmax"><code class="docutils literal notranslate"><span class="pre">softmax()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.activations.tanh"><code class="docutils literal notranslate"><span class="pre">tanh()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="neograd.autograd.ops.html#module-neograd.autograd.ops.basics">neograd.autograd.ops.basics module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.basics.Add"><code class="docutils literal notranslate"><span class="pre">Add</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.basics.Add.backward"><code class="docutils literal notranslate"><span class="pre">Add.backward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.basics.Add.forward"><code class="docutils literal notranslate"><span class="pre">Add.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.basics.Div"><code class="docutils literal notranslate"><span class="pre">Div</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.basics.Div.backward"><code class="docutils literal notranslate"><span class="pre">Div.backward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.basics.Div.forward"><code class="docutils literal notranslate"><span class="pre">Div.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.basics.Dot"><code class="docutils literal notranslate"><span class="pre">Dot</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.basics.Dot.backward"><code class="docutils literal notranslate"><span class="pre">Dot.backward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.basics.Dot.forward"><code class="docutils literal notranslate"><span class="pre">Dot.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.basics.Exp"><code class="docutils literal notranslate"><span class="pre">Exp</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.basics.Exp.backward"><code class="docutils literal notranslate"><span class="pre">Exp.backward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.basics.Exp.forward"><code class="docutils literal notranslate"><span class="pre">Exp.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.basics.Flatten"><code class="docutils literal notranslate"><span class="pre">Flatten</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.basics.Flatten.backward"><code class="docutils literal notranslate"><span class="pre">Flatten.backward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.basics.Flatten.forward"><code class="docutils literal notranslate"><span class="pre">Flatten.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.basics.Log"><code class="docutils literal notranslate"><span class="pre">Log</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.basics.Log.backward"><code class="docutils literal notranslate"><span class="pre">Log.backward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.basics.Log.forward"><code class="docutils literal notranslate"><span class="pre">Log.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.basics.Mul"><code class="docutils literal notranslate"><span class="pre">Mul</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.basics.Mul.backward"><code class="docutils literal notranslate"><span class="pre">Mul.backward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.basics.Mul.forward"><code class="docutils literal notranslate"><span class="pre">Mul.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.basics.Pow"><code class="docutils literal notranslate"><span class="pre">Pow</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.basics.Pow.backward"><code class="docutils literal notranslate"><span class="pre">Pow.backward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.basics.Pow.forward"><code class="docutils literal notranslate"><span class="pre">Pow.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.basics.Reshape"><code class="docutils literal notranslate"><span class="pre">Reshape</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.basics.Reshape.backward"><code class="docutils literal notranslate"><span class="pre">Reshape.backward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.basics.Reshape.forward"><code class="docutils literal notranslate"><span class="pre">Reshape.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.basics.Sub"><code class="docutils literal notranslate"><span class="pre">Sub</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.basics.Sub.backward"><code class="docutils literal notranslate"><span class="pre">Sub.backward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.basics.Sub.forward"><code class="docutils literal notranslate"><span class="pre">Sub.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.basics.Sum"><code class="docutils literal notranslate"><span class="pre">Sum</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.basics.Sum.backward"><code class="docutils literal notranslate"><span class="pre">Sum.backward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.basics.Sum.forward"><code class="docutils literal notranslate"><span class="pre">Sum.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.basics.Transpose"><code class="docutils literal notranslate"><span class="pre">Transpose</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.basics.Transpose.backward"><code class="docutils literal notranslate"><span class="pre">Transpose.backward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.basics.Transpose.forward"><code class="docutils literal notranslate"><span class="pre">Transpose.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.basics.add"><code class="docutils literal notranslate"><span class="pre">add()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.basics.div"><code class="docutils literal notranslate"><span class="pre">div()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.basics.dot"><code class="docutils literal notranslate"><span class="pre">dot()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.basics.exp"><code class="docutils literal notranslate"><span class="pre">exp()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.basics.flatten"><code class="docutils literal notranslate"><span class="pre">flatten()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.basics.log"><code class="docutils literal notranslate"><span class="pre">log()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.basics.mul"><code class="docutils literal notranslate"><span class="pre">mul()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.basics.pow"><code class="docutils literal notranslate"><span class="pre">pow()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.basics.reshape"><code class="docutils literal notranslate"><span class="pre">reshape()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.basics.sub"><code class="docutils literal notranslate"><span class="pre">sub()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.basics.sum"><code class="docutils literal notranslate"><span class="pre">sum()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.basics.transpose"><code class="docutils literal notranslate"><span class="pre">transpose()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="neograd.autograd.ops.html#module-neograd.autograd.ops.conv">neograd.autograd.ops.conv module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.conv.Conv"><code class="docutils literal notranslate"><span class="pre">Conv</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.conv.Conv.fragment_iterator"><code class="docutils literal notranslate"><span class="pre">Conv.fragment_iterator()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.conv.Conv.generate_fragments"><code class="docutils literal notranslate"><span class="pre">Conv.generate_fragments()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.conv.Conv.get_result_shape"><code class="docutils literal notranslate"><span class="pre">Conv.get_result_shape()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.conv.Conv.pad"><code class="docutils literal notranslate"><span class="pre">Conv.pad()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.conv.Conv.unpad"><code class="docutils literal notranslate"><span class="pre">Conv.unpad()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.conv.Conv2D"><code class="docutils literal notranslate"><span class="pre">Conv2D</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.conv.Conv2D.backward"><code class="docutils literal notranslate"><span class="pre">Conv2D.backward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.conv.Conv2D.forward"><code class="docutils literal notranslate"><span class="pre">Conv2D.forward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.conv.Conv2D.validate_inputs"><code class="docutils literal notranslate"><span class="pre">Conv2D.validate_inputs()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.conv.Conv3D"><code class="docutils literal notranslate"><span class="pre">Conv3D</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.conv.Conv3D.backward"><code class="docutils literal notranslate"><span class="pre">Conv3D.backward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.conv.Conv3D.forward"><code class="docutils literal notranslate"><span class="pre">Conv3D.forward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.conv.Conv3D.validate_inputs"><code class="docutils literal notranslate"><span class="pre">Conv3D.validate_inputs()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.conv.MaxPool2D"><code class="docutils literal notranslate"><span class="pre">MaxPool2D</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.conv.MaxPool2D.backward"><code class="docutils literal notranslate"><span class="pre">MaxPool2D.backward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.conv.MaxPool2D.forward"><code class="docutils literal notranslate"><span class="pre">MaxPool2D.forward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.conv.MaxPool2D.validate_inputs"><code class="docutils literal notranslate"><span class="pre">MaxPool2D.validate_inputs()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.conv.MaxPool3D"><code class="docutils literal notranslate"><span class="pre">MaxPool3D</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.conv.MaxPool3D.backward"><code class="docutils literal notranslate"><span class="pre">MaxPool3D.backward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.conv.MaxPool3D.forward"><code class="docutils literal notranslate"><span class="pre">MaxPool3D.forward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.conv.MaxPool3D.validate_inputs"><code class="docutils literal notranslate"><span class="pre">MaxPool3D.validate_inputs()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.conv.conv2d"><code class="docutils literal notranslate"><span class="pre">conv2d()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.conv.conv3d"><code class="docutils literal notranslate"><span class="pre">conv3d()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.conv.maxpool2d"><code class="docutils literal notranslate"><span class="pre">maxpool2d()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.conv.maxpool3d"><code class="docutils literal notranslate"><span class="pre">maxpool3d()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="neograd.autograd.ops.html#module-neograd.autograd.ops.operation">neograd.autograd.ops.operation module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.operation.Operation"><code class="docutils literal notranslate"><span class="pre">Operation</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.operation.Operation.backward"><code class="docutils literal notranslate"><span class="pre">Operation.backward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.operation.Operation.get_broadcast_shape"><code class="docutils literal notranslate"><span class="pre">Operation.get_broadcast_shape()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.operation.Operation.get_result_tensor"><code class="docutils literal notranslate"><span class="pre">Operation.get_result_tensor()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.operation.Operation.get_tensors"><code class="docutils literal notranslate"><span class="pre">Operation.get_tensors()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.operation.Operation.graph"><code class="docutils literal notranslate"><span class="pre">Operation.graph</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.operation.Operation.process_operands"><code class="docutils literal notranslate"><span class="pre">Operation.process_operands()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neograd.autograd.ops.html#neograd.autograd.ops.operation.Operation.result_requires_grad"><code class="docutils literal notranslate"><span class="pre">Operation.result_requires_grad()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="neograd.autograd.ops.html#module-neograd.autograd.ops">Module contents</a></li>
</ul>
</li>
</ul>
</div>
</section>
<section id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this heading">¶</a></h2>
</section>
<section id="module-neograd.autograd.graph">
<span id="neograd-autograd-graph-module"></span><h2>neograd.autograd.graph module<a class="headerlink" href="#module-neograd.autograd.graph" title="Permalink to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="neograd.autograd.graph.Graph">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neograd.autograd.graph.</span></span><span class="sig-name descname"><span class="pre">Graph</span></span><a class="headerlink" href="#neograd.autograd.graph.Graph" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Used to keep track of nodes and tensors</p>
<p>The graph is constructed during the forward pass, and used by the backward
pass to calculate gradients through automatic differentiation</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>nodes_dict</strong> (<em>dict</em>) – Stores key-value pairs of tensors and their corresponding
nodes in the graph</p></li>
<li><p><strong>track</strong> (<em>bool</em>) – Whether the graph must track the tensor operations or not, ie if True, when any
operation happens and a new result tensor is created, then the operands of the operation
are added as parents to the result tensor and the result tensor is added as child to the
operands, if False, none of these happens. Defaults to True</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="neograd.autograd.graph.Graph.add_edge">
<span class="sig-name descname"><span class="pre">add_edge</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">result_node</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">operands</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neograd.autograd.graph.Graph.add_edge" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates an edge between two nodes</p>
<p>Adds edges between the result_node, which is created during an Operation, and the
operands that produced the result. This means the result_node is added as a child of
each of the operands and the result_node adds all operands as its parents</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>result_node</strong> (<a class="reference internal" href="#neograd.autograd.node.Node" title="neograd.autograd.node.Node"><em>Node</em></a>) – node that is created in Operation.get_result_tensor</p></li>
<li><p><strong>operands</strong> (<em>list</em><em> of </em><a class="reference internal" href="#neograd.autograd.tensor.Tensor" title="neograd.autograd.tensor.Tensor"><em>Tensor</em></a>) – All the operands for an Operation</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neograd.autograd.graph.Graph.add_node">
<span class="sig-name descname"><span class="pre">add_node</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">node</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neograd.autograd.graph.Graph.add_node" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a Node to the graph</p>
<p>Creates an key-value pair in nodes_dict with the specified node as the value
and its tens attribute as the key</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>node</strong> (<a class="reference internal" href="#neograd.autograd.node.Node" title="neograd.autograd.node.Node"><em>Node</em></a>) – Node to be added to the graph</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neograd.autograd.graph.Graph.add_tensor">
<span class="sig-name descname"><span class="pre">add_tensor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tens</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neograd.autograd.graph.Graph.add_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a Tensor to the graph</p>
<p>A new node is created for the Tensor and corresponding entry is made
in nodes_dict</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>tens</strong> (<a class="reference internal" href="#neograd.autograd.tensor.Tensor" title="neograd.autograd.tensor.Tensor"><em>Tensor</em></a>) – Tensor to be added</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neograd.autograd.graph.Graph.get_node">
<span class="sig-name descname"><span class="pre">get_node</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tens</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neograd.autograd.graph.Graph.get_node" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the Node corresponding to the Tensor</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>tens</strong> (<a class="reference internal" href="#neograd.autograd.tensor.Tensor" title="neograd.autograd.tensor.Tensor"><em>Tensor</em></a>) – Tensor whose node is to be fetched</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Node if found, else None</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neograd.autograd.graph.Graph.remove_tensor">
<span class="sig-name descname"><span class="pre">remove_tensor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tens</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neograd.autograd.graph.Graph.remove_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Removes a Tensor from the graph</p>
<p>Pops the Tensor from nodes_dict</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>tens</strong> (<a class="reference internal" href="#neograd.autograd.tensor.Tensor" title="neograd.autograd.tensor.Tensor"><em>Tensor</em></a>) – Tensor to be removed</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neograd.autograd.graph.Graph.reset_graph">
<span class="sig-name descname"><span class="pre">reset_graph</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#neograd.autograd.graph.Graph.reset_graph" title="Permalink to this definition">¶</a></dt>
<dd><p>Resets the whole graph</p>
<p>This is accomplished by setting nodes_dict to an empty dictionary
Doing so, removes all the Tensors and their Nodes from the graph</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neograd.autograd.graph.Graph.reset_visited">
<span class="sig-name descname"><span class="pre">reset_visited</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#neograd.autograd.graph.Graph.reset_visited" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets visited=False for each Node in the graph</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neograd.autograd.graph.Graph.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#neograd.autograd.graph.Graph.zero_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs zero_grad on all the tensors in the graph</p>
<p>Iterates through nodes_dict and performs zero_grad on the tensors</p>
</dd></dl>

</dd></dl>

</section>
<section id="module-neograd.autograd.node">
<span id="neograd-autograd-node-module"></span><h2>neograd.autograd.node module<a class="headerlink" href="#module-neograd.autograd.node" title="Permalink to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="neograd.autograd.node.Node">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neograd.autograd.node.</span></span><span class="sig-name descname"><span class="pre">Node</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tens</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neograd.autograd.node.Node" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Used as an abstraction to connect the tensors together and hold relationships</p>
<p>Each Tensor is assigned a Node and this Node monitors all the incoming edges(parents)
and the outgoing edges(children)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>children</strong> (<em>list</em><em> of </em><a class="reference internal" href="#neograd.autograd.node.Node" title="neograd.autograd.node.Node"><em>Node</em></a>) – List of all Nodes which uses the current Node
as an operand in an Operation</p></li>
<li><p><strong>parents</strong> (<em>list</em><em> of </em><a class="reference internal" href="#neograd.autograd.node.Node" title="neograd.autograd.node.Node"><em>Node</em></a>) – List of all Nodes(operands) that has resulted in the creation
of current Node</p></li>
<li><p><strong>parent_broadcast_shape</strong> (<em>tuple</em><em> or </em><em>None</em>) – If the parent needs to be broadcasted from one shape to
another, then the final broadcasted shape of the parent is stored here.
If they cannot be broadcasted, then it is None</p></li>
<li><p><strong>backward_fn</strong> (<em>Operation.backward</em>) – Sets the grad_fn of Tensor(operand) involved in the Operation</p></li>
<li><p><strong>visited</strong> (<em>bool</em>) – </p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="neograd.autograd.node.Node.add_child">
<span class="sig-name descname"><span class="pre">add_child</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neograd.autograd.node.Node.add_child" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a child to the Node</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>other</strong> (<a class="reference internal" href="#neograd.autograd.node.Node" title="neograd.autograd.node.Node"><em>Node</em></a>) – The child Node</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neograd.autograd.node.Node.add_parent">
<span class="sig-name descname"><span class="pre">add_parent</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neograd.autograd.node.Node.add_parent" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a parent to the Node</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>other</strong> (<a class="reference internal" href="#neograd.autograd.node.Node" title="neograd.autograd.node.Node"><em>Node</em></a>) – The parent Node</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neograd.autograd.node.Node.are_children_visited">
<span class="sig-name descname"><span class="pre">are_children_visited</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#neograd.autograd.node.Node.are_children_visited" title="Permalink to this definition">¶</a></dt>
<dd><p>Checks if all children are visited</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>True if all children are visited else False</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neograd.autograd.node.Node.are_parents_visited">
<span class="sig-name descname"><span class="pre">are_parents_visited</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#neograd.autograd.node.Node.are_parents_visited" title="Permalink to this definition">¶</a></dt>
<dd><p>Checks if all parents are visited</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>True if all parents are visited else False</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neograd.autograd.node.Node.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">retain_graph</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neograd.autograd.node.Node.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Initiates backward pass starting from current Node</p>
<p>This first topologically sorts all Tensors starting from current Node, then the Node
corresponding to the Tensor is retreived, which is marked as visited and the Tensor’s
backward pass is initiated.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>retain_graph</strong> (<em>bool</em>) – If the graph should be retained after backward pass or flushed
after backward calculation</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neograd.autograd.node.Node.top_sort">
<span class="sig-name descname"><span class="pre">top_sort</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#neograd.autograd.node.Node.top_sort" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs topological sort of all Nodes starting from current Node</p>
<p>Sorts the graph topologically, to perform backward pass efficiently, so that all the children’s
is calculated before the current node’s gradient is calculated.</p>
<p>Sorting is done by first checking if all the children are visited, if they are, then the current
node is added to sorted_tensors if not, then topological sort is performed on children</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neograd.autograd.node.Node.visit_all_children">
<span class="sig-name descname"><span class="pre">visit_all_children</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#neograd.autograd.node.Node.visit_all_children" title="Permalink to this definition">¶</a></dt>
<dd><p>Marks all children as visited</p>
</dd></dl>

</dd></dl>

</section>
<section id="module-neograd.autograd.tensor">
<span id="neograd-autograd-tensor-module"></span><h2>neograd.autograd.tensor module<a class="headerlink" href="#module-neograd.autograd.tensor" title="Permalink to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="neograd.autograd.tensor.Tensor">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neograd.autograd.tensor.</span></span><span class="sig-name descname"><span class="pre">Tensor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">requires_broadcasting</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neograd.autograd.tensor.Tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Wrapper around NumPy arrays</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<em>int</em><em> or </em><em>float</em><em> or </em><em>list</em><em> or </em><em>np.ndarray</em>) – The data to be stored or manipulated</p></li>
<li><p><strong>requires_grad</strong> (<em>bool</em>) – Whether the Tensor requires gradient to be calculated or not
Defaults to False</p></li>
<li><p><strong>requires_broadcasting</strong> (<em>bool</em>) – Whether the Tensor needs to be broadcasted when some Operation
is performed. Defaults to True. This attribute is present as there are some operations like
Convolution for which the kernel shouldn’t be broadcasted to inputs shape</p></li>
<li><p><strong>grad</strong> (<em>np.ndarray</em>) – The gradient value of the Tensor. Defaults to 0 if requires_grad else None</p></li>
<li><p><strong>grad_fn</strong> – The function that is set in Operation.backward, that’ll be executed during backward
pass to set the gradient of the Tensor</p></li>
</ul>
</dd>
</dl>
<dl class="py property">
<dt class="sig sig-object py" id="neograd.autograd.tensor.Tensor.T">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">T</span></span><a class="headerlink" href="#neograd.autograd.tensor.Tensor.T" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs the transpose of the Tensor</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>Transpose of the Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neograd.autograd.tensor.Tensor._backward">
<span class="sig-name descname"><span class="pre">_backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">node</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retain_graph</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neograd.autograd.tensor.Tensor._backward" title="Permalink to this definition">¶</a></dt>
<dd><p>The essence of autograd, final gradient calculations for the Tensor is performed here</p>
<p>The gradient of each child is taken as upper gradient, the backward_fn of the
Node of the Tensor is executed to set the grad_fn of Tensor.</p>
<p>grad_fn is executed, the grad is then unbroadcasted, if Tensor has been broadcasted
during the Operation. auto-removal of Tensor from the graph is performed when
retain_graph is False</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>node</strong> (<a class="reference internal" href="#neograd.autograd.node.Node" title="neograd.autograd.node.Node"><em>Node</em></a>) – The Node corresponding to the Tensor</p></li>
<li><p><strong>retain_graph</strong> (<em>bool</em>) – Whether the graph needs to be retained or reset</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neograd.autograd.tensor.Tensor.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">upper_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retain_graph</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neograd.autograd.tensor.Tensor.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Kicks off the backward pass to calculate gradients</p>
<p>Starts the gradient calculation for the backward pass from the
Tensor, by calling the backward method of its corresponding Node</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>upper_grad</strong> (<em>int</em><em> or </em><em>float</em><em> or </em><em>list</em><em> or </em><em>np.ndarray</em>) – The gradient with which to start the
gradient calculation. Shape of upper_grad and shape of Tensor must be the same.
Defaults to 1 as usually backward is called on a loss Tensor that has a scalar value</p></li>
<li><p><strong>retain_graph</strong> (<em>bool</em>) – If the graph should be retained after backward pass or should be reset.
Auto-removal of Tensors from the graph, which happens when the gradients of all tensors of
node’s parents have been calculated will be turned off.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>ValueError</strong> – If called on a Tensor that doesn’t have requires_grad</p></li>
<li><p><strong>ValueError</strong> – If shapes of upper_grad and Tensor doesn’t match</p></li>
<li><p><strong>ValueError</strong> – If backward is called on a Tensor, whose children aren’t 0, ie
    they are further involved in some other operations and aren’t the leaves</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="neograd.autograd.tensor.Tensor.data">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">data</span></span><a class="headerlink" href="#neograd.autograd.tensor.Tensor.data" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the data present in the Tensor</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>Data in the Tensor</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>data (np.ndarray)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neograd.autograd.tensor.Tensor.dot">
<span class="sig-name descname"><span class="pre">dot</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neograd.autograd.tensor.Tensor.dot" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs dot product of Tensor with another object</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>other</strong> (<em>int</em><em> or </em><em>float</em><em> or </em><em>list</em><em> or </em><em>np.ndarray</em>) – The object that needs to be dotted with</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Tensor of the result</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neograd.autograd.tensor.Tensor.exp">
<span class="sig-name descname"><span class="pre">exp</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#neograd.autograd.tensor.Tensor.exp" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs exponentiation on the Tensor</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tensor of the result</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neograd.autograd.tensor.Tensor.flatten">
<span class="sig-name descname"><span class="pre">flatten</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#neograd.autograd.tensor.Tensor.flatten" title="Permalink to this definition">¶</a></dt>
<dd><p>Flattens the Tensor from any dimension to 1D</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>Flattened Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neograd.autograd.tensor.Tensor.reshape">
<span class="sig-name descname"><span class="pre">reshape</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">new_shape</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neograd.autograd.tensor.Tensor.reshape" title="Permalink to this definition">¶</a></dt>
<dd><p>Reshapes the Tensor to the new shape</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>new_shape</strong> (<em>tuple</em>) – The shape to which the Tensor should be reshaped to</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Reshaped Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neograd.autograd.tensor.Tensor.set_grad_fn">
<span class="sig-name descname"><span class="pre">set_grad_fn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">grad_fn</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neograd.autograd.tensor.Tensor.set_grad_fn" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the grad_fn for the Tensor</p>
<p>If requires_grad is True, then Tensor.grad_fn is set to grad_fn else None</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>grad_fn</strong> – Function that is set during execution of Operation.backward</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="neograd.autograd.tensor.Tensor.shape">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">shape</span></span><a class="headerlink" href="#neograd.autograd.tensor.Tensor.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the shape of the Tensor</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>Shape of data in the Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neograd.autograd.tensor.Tensor.sum">
<span class="sig-name descname"><span class="pre">sum</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neograd.autograd.tensor.Tensor.sum" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs sum of Tensor along an axis</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>axis</strong> (<em>None</em><em> or </em><em>int</em>) – The axis along which it should be summed</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Tensor of the result</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neograd.autograd.tensor.Tensor.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#neograd.autograd.tensor.Tensor.zero_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Resets the grad of the Tensor to the defaults</p>
</dd></dl>

</dd></dl>

</section>
<section id="module-neograd.autograd.utils">
<span id="neograd-autograd-utils-module"></span><h2>neograd.autograd.utils module<a class="headerlink" href="#module-neograd.autograd.utils" title="Permalink to this heading">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="neograd.autograd.utils._evaluate_grad_check">
<span class="sig-prename descclassname"><span class="pre">neograd.autograd.utils.</span></span><span class="sig-name descname"><span class="pre">_evaluate_grad_check</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">analytical_grads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">calculated_grads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">print_vals</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neograd.autograd.utils._evaluate_grad_check" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluates the gradient check and indicates whether it has passed or not</p>
<p>Calculates the distance between the analytical and calculated gradients and if it is
less than epsilon, then it has passed else failed</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>analytical_grads</strong> (<em>list</em><em> of </em><em>int</em><em> or </em><em>float</em>) – Gradients that are calculated analytically
by wiggling the parameters</p></li>
<li><p><strong>calculated_grads</strong> (<em>list</em><em> of </em><em>int</em><em> or </em><em>float</em>) – Gradients that are calulated through
backpropagation</p></li>
<li><p><strong>epsilon</strong> (<em>float</em>) – The amount by which params need to be wiggled</p></li>
<li><p><strong>print_vals</strong> (<em>bool</em>) – True if distance and verdict needs to be printed</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Distance between analytical and calculated gradients</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="neograd.autograd.utils._wiggle_params">
<span class="sig-prename descclassname"><span class="pre">neograd.autograd.utils.</span></span><span class="sig-name descname"><span class="pre">_wiggle_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">analytical_grads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">calculated_grads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">params</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">get_loss</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neograd.autograd.utils._wiggle_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Changes the params value by epsilon and calculates the analytical gradient</p>
<p>First to each element in params.data epsilon is added and loss is calculated, similarly
2*epsilon is subtracted to get another loss and using these two analytical gradient is calculated
and is appended to analytical_grads and the gradient in param is appended to calculated_grads</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>analytical_grads</strong> (<em>list</em><em> of </em><em>int</em><em> or </em><em>float</em>) – Gradients that are calculated analytically
by wiggling the parameters</p></li>
<li><p><strong>calculated_grads</strong> (<em>list</em><em> of </em><em>int</em><em> or </em><em>float</em>) – Gradients that are calulated through
backpropagation</p></li>
<li><p><strong>params</strong> (<em>list</em><em> of </em><a class="reference internal" href="#neograd.autograd.tensor.Tensor" title="neograd.autograd.tensor.Tensor"><em>Tensor</em></a>) – All params that need to be wiggled</p></li>
<li><p><strong>get_loss</strong> – function that is used to calculate the loss</p></li>
<li><p><strong>epsilon</strong> (<em>float</em>) – The amount by which params need to be wiggled</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="neograd.autograd.utils.fn_grad_check">
<span class="sig-prename descclassname"><span class="pre">neograd.autograd.utils.</span></span><span class="sig-name descname"><span class="pre">fn_grad_check</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">params</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">targets</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-07</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">print_vals</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neograd.autograd.utils.fn_grad_check" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs Gradient Check for a function</p>
<p>Implements Gradient Check for a function instead of a complete model
Any params that are required to be gradient checked can be specified</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fn</strong> – Function to be gradient checked</p></li>
<li><p><strong>inputs</strong> (<em>list</em><em> of </em><a class="reference internal" href="#neograd.autograd.tensor.Tensor" title="neograd.autograd.tensor.Tensor"><em>Tensor</em></a>) – inputs to the function</p></li>
<li><p><strong>params</strong> (<em>list</em><em> of </em><a class="reference internal" href="#neograd.autograd.tensor.Tensor" title="neograd.autograd.tensor.Tensor"><em>Tensor</em></a>) – the params whose data can be wiggled to get the gradients</p></li>
<li><p><strong>targets</strong> (<a class="reference internal" href="#neograd.autograd.tensor.Tensor" title="neograd.autograd.tensor.Tensor"><em>Tensor</em></a>) – targets of the function</p></li>
<li><p><strong>loss_fn</strong> (<a class="reference internal" href="neograd.nn.html#neograd.nn.loss.Loss" title="neograd.nn.loss.Loss"><em>Loss</em></a>) – loss_fn to evaluate the function</p></li>
<li><p><strong>epsilon</strong> (<em>float</em>) – The amount by which params need to be wiggled Defaults to 1e-7</p></li>
<li><p><strong>print_vals</strong> (<em>bool</em>) – True if distance and verdict needs to be printed</p></li>
<li><p><strong>**kwargs</strong> – Any kwargs to be passed to fn</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Distance between analytical and calculated gradients</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="neograd.autograd.utils.get_dims_to_be_summed">
<span class="sig-prename descclassname"><span class="pre">neograd.autograd.utils.</span></span><span class="sig-name descname"><span class="pre">get_dims_to_be_summed</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">orig_data_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">broadcasted_shape</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neograd.autograd.utils.get_dims_to_be_summed" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the dimensions along which data has been broadcasted</p>
<p>Given the original data shape and its broadcasted shape, it returns True along
a dimension if their dimensions don’t match, else returns False if they match,
meaning there has been no broadcasting along that dimension.
<a class="reference external" href="https://numpy.org/doc/stable/user/basics.broadcasting.html">https://numpy.org/doc/stable/user/basics.broadcasting.html</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>orig_data_shape</strong> (<em>tuple</em>) – Original shape of data before broadcasting</p></li>
<li><p><strong>broadcasted_shape</strong> (<em>tuple</em>) – Shape to which data has been broadcasted to</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>list of bool values, True if there’s broadcasting, False if there isn’t</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="neograd.autograd.utils.get_graph">
<span class="sig-prename descclassname"><span class="pre">neograd.autograd.utils.</span></span><span class="sig-name descname"><span class="pre">get_graph</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#neograd.autograd.utils.get_graph" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns graph that is in use and present in Operation.graph</p>
<p>If Operation.graph is None, then the global graph _NG_GRAPH is used</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>Graph object that is currently used</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="neograd.autograd.utils.grad_check">
<span class="sig-prename descclassname"><span class="pre">neograd.autograd.utils.</span></span><span class="sig-name descname"><span class="pre">grad_check</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">targets</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_fn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-07</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">print_vals</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neograd.autograd.utils.grad_check" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs Gradient Check</p>
<p>Implements Gradient Check, to make sure that backprop is calculating
the right gradients. All the parameters in the model are checked.</p>
<p>If distance between backprop gradients and numerical gradients is less
than epsilon, then the gradients are proper, if not there is an issue</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<a class="reference internal" href="neograd.nn.html#neograd.nn.model.Model" title="neograd.nn.model.Model"><em>Model</em></a>) – The Neural Network to be evaluated</p></li>
<li><p><strong>inputs</strong> (<a class="reference internal" href="#neograd.autograd.tensor.Tensor" title="neograd.autograd.tensor.Tensor"><em>Tensor</em></a>) – Input data(No need for complete data, only sample enough)</p></li>
<li><p><strong>targets</strong> (<a class="reference internal" href="#neograd.autograd.tensor.Tensor" title="neograd.autograd.tensor.Tensor"><em>Tensor</em></a>) – Targets</p></li>
<li><p><strong>loss_fn</strong> (<a class="reference internal" href="neograd.nn.html#neograd.nn.loss.Loss" title="neograd.nn.loss.Loss"><em>Loss</em></a>) – Loss Function</p></li>
<li><p><strong>epsilon</strong> (<em>float</em>) – The amount by which params need to be wiggled Defaults to 1e-7</p></li>
<li><p><strong>print_vals</strong> (<em>bool</em>) – True if distance and verdict needs to be printed</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Distance between analytical and calculated gradients</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="neograd.autograd.utils.new_graph">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neograd.autograd.utils.</span></span><span class="sig-name descname"><span class="pre">new_graph</span></span><a class="headerlink" href="#neograd.autograd.utils.new_graph" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Creates a Graph object</p>
<p>Context Manager to create a new graph if required anywhere and under the
circumstances where it shouldn’t interfere with the global _NG_GRAPH</p>
<p>After entering, Graph object created is set in Operation.graph. After exiting
the Operation.graph is set back to None which implies that global _NG_GRAPH will
be used</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="neograd.autograd.utils.no_track">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neograd.autograd.utils.</span></span><span class="sig-name descname"><span class="pre">no_track</span></span><a class="headerlink" href="#neograd.autograd.utils.no_track" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Prevents tracking of Tensors</p>
<p>Context Manager to prevent creation of a backward graph, when gradient calculation
is not required, for ex when testing a model after training it, you don’t need
any backward pass</p>
<p>On entering, graph.track is set to False to indicate no tracking and on exiting, it is
set back to True</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>graph</strong> (<a class="reference internal" href="#neograd.autograd.graph.Graph" title="neograd.autograd.graph.Graph"><em>Graph</em></a>) – The current graph in use</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="neograd.autograd.utils.process_data">
<span class="sig-prename descclassname"><span class="pre">neograd.autograd.utils.</span></span><span class="sig-name descname"><span class="pre">process_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neograd.autograd.utils.process_data" title="Permalink to this definition">¶</a></dt>
<dd><p>Checks and processes the data for storage in Tensor</p>
<p>Supported types for data - [int, float, list, np.ndarray]
Elements in data should be float or be typecastable to float</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>data</strong> (<em>int</em><em> or </em><em>float</em><em> or </em><em>list</em><em> or </em><em>np.ndarray</em>) – Data to be processed</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Processed data</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>TypeError</strong> – If data or its elements aren’t typecastable to float</p></li>
<li><p><strong>TypeError</strong> – If data is not instance of supported types</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="neograd.autograd.utils.unbroadcast_data">
<span class="sig-prename descclassname"><span class="pre">neograd.autograd.utils.</span></span><span class="sig-name descname"><span class="pre">unbroadcast_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">orig_data_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">broadcasted_shape</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neograd.autograd.utils.unbroadcast_data" title="Permalink to this definition">¶</a></dt>
<dd><p>Unbroadcasts the data to its original shape</p>
<p>If data(a np object) is broadcasted during an operation, then it is unbroadcasted here
where all dimensions where it was broadcasted are summed along that dimension to
give the original shape of the data. If broadcasted_shape is None, then the data is
returned as is.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<em>np.ndarray</em>) – Data to be unbroadcasted</p></li>
<li><p><strong>orig_data_shape</strong> (<em>tuple</em>) – Original shape of data before broadcasting</p></li>
<li><p><strong>broadcasted_shape</strong> (<em>tuple</em>) – Shape to which data has been broadcasted to</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Data that is unbroadcasted</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-neograd.autograd">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-neograd.autograd" title="Permalink to this heading">¶</a></h2>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">neograd</a></h1>








<h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="modules.html">neograd</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="neograd.html">neograd package</a></li>
</ul>
</li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
  <li><a href="modules.html">neograd</a><ul>
  <li><a href="neograd.html">neograd package</a><ul>
      <li>Previous: <a href="neograd.html" title="previous chapter">neograd package</a></li>
      <li>Next: <a href="neograd.autograd.ops.html" title="next chapter">neograd.autograd.ops package</a></li>
  </ul></li>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2022, Pranav Sastry.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 5.2.3</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/neograd.autograd.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>