{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute when running on Google Colab\n",
    "!pip install neograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute when running locally\n",
    "# import sys\n",
    "# sys.path.insert(0, '../tests')\n",
    "# import _setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neograd as ng\n",
    "import numpy as np\n",
    "from neograd.nn.loss import SoftmaxCE\n",
    "from neograd.nn.optim import Adam\n",
    "from neograd.autograd.utils import grad_check\n",
    "from neograd.nn.utils import get_batches\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_digits(return_X_y=True) # load data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y) # split data into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train, num_test = X_train.shape[0], X_test.shape[0] # number of train and test examples\n",
    "num_iter = 200 # number of iterations\n",
    "batch_size = 200 # batch size in each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize inputs\n",
    "X_train_norm = (X_train - np.mean(X_train, axis=1, keepdims=True))/np.std(X_train, axis=1, keepdims=True)\n",
    "X_test_norm = (X_test - np.mean(X_test, axis=1, keepdims=True))/np.std(X_test, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert normalized inputs and targets to Tensor\n",
    "X_train = ng.tensor(X_train_norm[:num_train,:].reshape(num_train,8,8))\n",
    "X_test = ng.tensor(X_test_norm[:num_test,:].reshape(num_test,8,8))\n",
    "y_train = ng.tensor(np.eye(10)[y_train[:num_train]])\n",
    "y_test = ng.tensor(y_test[:num_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class NN(ng.nn.Model):\n",
    "  def __init__(self):\n",
    "    self.conv = ng.nn.Sequential(\n",
    "      ng.nn.Conv2D((3,3)),\n",
    "      ng.nn.ReLU()\n",
    "    )\n",
    "    self.stack = ng.nn.Sequential(\n",
    "        ng.nn.Linear(36,10)\n",
    "    )\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    conv_outputs = self.conv(inputs)\n",
    "    conv_outputs_flattened = conv_outputs.reshape((inputs.shape[0], 36))\n",
    "    return self.stack(conv_outputs_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure the model\n",
    "model = NN()\n",
    "loss_fn = SoftmaxCE(axis=1)\n",
    "optim = Adam(model.get_params(), 5e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1/200\n",
      "loss: Tensor( 11.544053028208827,\n",
      " requires_grad=True,\n",
      " grad_fn=None,\n",
      " shape=() )\n",
      "\n",
      "\n",
      "iter 51/200\n",
      "loss: Tensor( 0.45721614541777483,\n",
      " requires_grad=True,\n",
      " grad_fn=None,\n",
      " shape=() )\n",
      "\n",
      "\n",
      "iter 101/200\n",
      "loss: Tensor( 0.15749437587925777,\n",
      " requires_grad=True,\n",
      " grad_fn=None,\n",
      " shape=() )\n",
      "\n",
      "\n",
      "iter 151/200\n",
      "loss: Tensor( 0.08086855586332076,\n",
      " requires_grad=True,\n",
      " grad_fn=None,\n",
      " shape=() )\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "for iter in range(num_iter):\n",
    "  for batch_input, batch_target in get_batches(X_train, y_train, batch_size):\n",
    "    optim.zero_grad()\n",
    "    outputs = model(batch_input)\n",
    "    loss = loss_fn(outputs, batch_target)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "  if iter%50==0:\n",
    "    print(f\"iter {iter+1}/{num_iter}\\nloss: {loss}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "with model.eval():\n",
    "  test_outputs = model(X_test)\n",
    "  probs = ng.nn.Softmax(1)(test_outputs.data)\n",
    "  preds = np.argmax(probs.data, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99        43\n",
      "           1       0.93      0.91      0.92        46\n",
      "           2       0.93      0.91      0.92        44\n",
      "           3       0.91      0.88      0.89        48\n",
      "           4       0.94      0.90      0.92        50\n",
      "           5       0.87      0.98      0.92        41\n",
      "           6       0.98      0.96      0.97        48\n",
      "           7       0.95      1.00      0.97        37\n",
      "           8       0.78      0.89      0.83        44\n",
      "           9       0.98      0.84      0.90        49\n",
      "\n",
      "    accuracy                           0.92       450\n",
      "   macro avg       0.92      0.93      0.92       450\n",
      "weighted avg       0.93      0.92      0.92       450\n",
      "\n",
      "Accuracy: 0.9222222222222223\n"
     ]
    }
   ],
   "source": [
    "# get report\n",
    "report = classification_report(y_test.data.astype(int).flatten(), preds.flatten())\n",
    "print(report)\n",
    "accuracy = accuracy_score(y_test.data.astype(int).flatten(), preds.flatten())\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Check Distance: 1.968236460958225e-08\n",
      "Gradient Check PASSED\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.968236460958225e-08"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform gradient check on the model\n",
    "grad_check(model, X_train, y_train, loss_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
